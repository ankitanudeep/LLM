## 🧠 Ollama - Run LLMs Locally

**Ollama** is a powerful and user-friendly tool that allows you to run large language models (LLMs) **locally on your own machine**. It supports popular open-source models like **LLaMA 3**, **Mistral**, **Gemma**, **Phi**, and more.

### 🚀 Why Use Ollama?
- **Run models locally** without internet once downloaded.
- **No API keys** or cloud access required.
- Useful for **offline development**, **privacy-sensitive projects**, and quick experimentation.
- Supports **custom models** and **model fine-tuning**.

### 🆓 Free & Open
Ollama is completely **free to use** and works with many **open-source models**.

### 🖥️ Local-First by Design
Once a model is pulled, it runs **entirely on your device**, using your CPU or GPU (optimized for Mac, Linux, Windows with WSL).

### 🔗 Learn More
👉 [https://ollama.com](https://ollama.com)

